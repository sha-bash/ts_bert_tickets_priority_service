model:
  name: 'DeepPavlov/rubert-base-cased'  # Предобученная русскоязычная модель
  dropout_rate: 0.2                    # Оптимально для средних датасетов

training:
  # Основные параметры
  num_train_epochs: 6                   # Уменьшено для предотвращения переобучения
  per_device_train_batch_size: 24       
  per_device_eval_batch_size: 48        # Быстрая оценка
  learning_rate: 3e-5                   # Слегка увеличен для русского BERT
  warmup_ratio: 0.1                     # Более гибкий контроль прогрева
  weight_decay: 0.02                    # Усиленная регуляризация
  
  # Градиентный аккумулирование
  gradient_accumulation_steps: 2       
  
  # Оптимизация памяти
  fp16: True                           # Активация mixed-precision
  gradient_checkpointing: True         # Экономия памяти
  
  # Расписание обучения
  lr_scheduler_type: 'cosine'         # Плавное уменьшение LR
  
  # Мониторинг
  logging_steps: 50                   # Более частый мониторинг
  eval_strategy: 'epoch'              # Оценка каждые 100 шагов
  eval_steps: 100
  save_strategy: 'epoch'
  load_best_model_at_end: True
  metric_for_best_model: 'eval_f1'
  greater_is_better: True
  
  # Специфичные для русского языка
  max_seq_length: 256                # Увеличение для русских предложений